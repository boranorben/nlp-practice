{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3700284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from konlpy) (1.23.5)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from konlpy) (4.9.2)\n",
      "Requirement already satisfied: packaging in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
      "Requirement already satisfied: konlp in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (0.0.58)\n",
      "Requirement already satisfied: coveralls==1.1 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from konlp) (1.1)\n",
      "Requirement already satisfied: jpype1>=1.3.0 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from konlp) (1.4.1)\n",
      "Requirement already satisfied: pytest-cov==2.4.0 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from konlp) (2.4.0)\n",
      "Requirement already satisfied: requests>=1.0.0 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from coveralls==1.1->konlp) (2.28.1)\n",
      "Requirement already satisfied: coverage>=3.6 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from coveralls==1.1->konlp) (7.2.3)\n",
      "Requirement already satisfied: docopt>=0.6.1 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from coveralls==1.1->konlp) (0.6.2)\n",
      "Requirement already satisfied: pytest>=2.6.0 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from pytest-cov==2.4.0->konlp) (7.3.1)\n",
      "Requirement already satisfied: packaging in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from jpype1>=1.3.0->konlp) (23.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from pytest>=2.6.0->pytest-cov==2.4.0->konlp) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from pytest>=2.6.0->pytest-cov==2.4.0->konlp) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from pytest>=2.6.0->pytest-cov==2.4.0->konlp) (2.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from pytest>=2.6.0->pytest-cov==2.4.0->konlp) (1.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from requests>=1.0.0->coveralls==1.1->konlp) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from requests>=1.0.0->coveralls==1.1->konlp) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from requests>=1.0.0->coveralls==1.1->konlp) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from requests>=1.0.0->coveralls==1.1->konlp) (1.26.15)\n",
      "Requirement already satisfied: pyjnius in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: six>=1.7.0 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from pyjnius) (1.16.0)\n",
      "Requirement already satisfied: nltk in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (from nltk) (8.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy\n",
    "!pip install konlp\n",
    "!pip install pyjnius\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb138d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0. KLT2000 분석 결과\n",
      "['나/1', '눈/1', '보/V', '밤하늘/N', '별/1', '되/V', '기분/N', '느끼/V', '수/1', '있/V', '거/1']\n",
      "['나', '눈', '보', '밤하늘', '별', '되', '기분', '느끼', '수', '있', '거']\n",
      "['나', '눈', '밤하늘', '별', '기분', '수', '거']\n",
      "\n",
      "1. Okt 분석 결과\n",
      "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n",
      "['질문', '건의', '사항', '깃', '이슈', '트래커']\n",
      "['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행']\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나욬', 'Noun'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "\n",
      "2. Kkma 분석 결과\n",
      "['네 , 안녕하세요.', '반갑습니다.']\n",
      "['질문', '건의', '건의사항', '사항', '깃헙', '이슈', '트래커']\n",
      "[('오류', 'NNG'),\n",
      " ('보고', 'NNG'),\n",
      " ('는', 'JX'),\n",
      " ('실행', 'NNG'),\n",
      " ('환경', 'NNG'),\n",
      " (',', 'SP'),\n",
      " ('에러', 'NNG'),\n",
      " ('메세지', 'NNG'),\n",
      " ('와', 'JKM'),\n",
      " ('함께', 'MAG'),\n",
      " ('설명', 'NNG'),\n",
      " ('을', 'JKO'),\n",
      " ('최대한', 'NNG'),\n",
      " ('상세히', 'MAG'),\n",
      " ('!', 'SF'),\n",
      " ('^^', 'EMO')]\n",
      "\n",
      "3. Komoran 분석 결과\n",
      "['우왕', '코', '모란', '도', '오픈', '소스', '가', '되', '었', '어요']\n",
      "['질문', '건의', '사항', '이슈']\n",
      "[('혹시', 'MAG'), ('바람과 함께 사라지다', 'NNP'), ('보', 'VV'), ('았', 'EP'), ('어', 'EF'), ('?', 'SF')]\n",
      "\n",
      "4. Hannanum 분석 결과\n",
      "[[[('롯데마트', 'ncn'), ('의', 'jcm')], [('롯데마트의', 'ncn')], [('롯데마트', 'nqq'), ('의', 'jcm')], [('롯데마트의', 'nqq')]], [[('흑마늘', 'ncn')], [('흑마늘', 'nqq')]], [[('양념', 'ncn')]], [[('치킨', 'ncn'), ('이', 'jcc')], [('치킨', 'ncn'), ('이', 'jcs')], [('치킨', 'ncn'), ('이', 'ncn')]], [[('논란', 'ncpa'), ('이', 'jcc')], [('논란', 'ncpa'), ('이', 'jcs')], [('논란', 'ncpa'), ('이', 'ncn')]], [[('되', 'nbu'), ('고', 'jcj')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecc')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecs')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecx')], [('되', 'paa'), ('고', 'ecc')], [('되', 'paa'), ('고', 'ecs')], [('되', 'paa'), ('고', 'ecx')], [('되', 'pvg'), ('고', 'ecc')], [('되', 'pvg'), ('고', 'ecs')], [('되', 'pvg'), ('고', 'ecx')], [('되', 'px'), ('고', 'ecc')], [('되', 'px'), ('고', 'ecs')], [('되', 'px'), ('고', 'ecx')]], [[('있', 'paa'), ('다', 'ef')], [('있', 'px'), ('다', 'ef')]], [[('.', 'sf')], [('.', 'sy')]]]\n",
      "['롯데마트', '의', '흑마늘', '양념', '치킨', '이', '논란', '이', '되', '고', '있', '다', '.']\n",
      "['질문', '건의사항', '깃헙', '이슈', '트래커']\n",
      "[('웃', 'P'), ('으면', 'E'), ('더', 'M'), ('행복', 'N'), ('하', 'X'), ('ㅂ니다', 'E'), ('!', 'S')]\n"
     ]
    }
   ],
   "source": [
    "!python KoNLTKtest/konltk-test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7bf3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/boranorben/Documents/github/nlp-practice/hw4/KoNLTKtest\n"
     ]
    }
   ],
   "source": [
    "%cd KoNLTKtest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e9c600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec. time for Kkma = 367.037921667099\n",
      "Exec. time for OKT = 32.69651818275452\n",
      "Exec. time for KLT = 17.000275373458862\n",
      "KMA results are saved in output-KLT.txt output-Kkma.txt output-OKT.txt\n",
      "Exec. time for Kkma = 989.1626889705658\n",
      "Exec. time for OKT = 83.1450879573822\n",
      "Exec. time for KLT = 54.84168815612793\n",
      "KMA results are saved in output-KLT.txt output-Kkma.txt output-OKT.txt\n"
     ]
    }
   ],
   "source": [
    "!python testKMAspeed.py ITnews1000.txt\n",
    "!python testKMAspeed.py gtlee.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4545fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/boranorben/opt/anaconda3/envs/nlp-project/lib/python3.8/site-packages (0.1.98)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6431dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=ITnews1000.txt --model_prefix=ITnews --vocab_size=8000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ITnews1000.txt\n",
      "  input_format: \n",
      "  model_prefix: ITnews\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ITnews1000.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 21867 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=1240970\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=1480\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 15481 sentences.\n",
      "unigram_model_trainer.cc(247) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(251) LOG(INFO) Extracting frequent sub strings... node_num=482093\n",
      "unigram_model_trainer.cc(301) LOG(INFO) Initialized 143943 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 15481\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 90198\n",
      "unigram_model_trainer.cc(607) LOG(INFO) Using 90198 sentences for EM training\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=49182 obj=19.4681 num_tokens=206574 num_tokens/piece=4.2002\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=45859 obj=14.2066 num_tokens=203684 num_tokens/piece=4.44153\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=34387 obj=14.3998 num_tokens=218533 num_tokens/piece=6.35511\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=34356 obj=14.2803 num_tokens=218560 num_tokens/piece=6.36163\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=25766 obj=14.7307 num_tokens=234899 num_tokens/piece=9.11663\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=25766 obj=14.6117 num_tokens=234928 num_tokens/piece=9.11775\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=19324 obj=15.1243 num_tokens=250419 num_tokens/piece=12.959\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=19324 obj=15.0165 num_tokens=250427 num_tokens/piece=12.9594\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=14493 obj=15.5733 num_tokens=266464 num_tokens/piece=18.3857\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=14493 obj=15.4702 num_tokens=266469 num_tokens/piece=18.386\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=10869 obj=16.0814 num_tokens=282926 num_tokens/piece=26.0305\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=10869 obj=15.9749 num_tokens=282928 num_tokens/piece=26.0307\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=8800 obj=16.4504 num_tokens=295279 num_tokens/piece=33.5544\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=8800 obj=16.3733 num_tokens=295282 num_tokens/piece=33.5548\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ITnews.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ITnews.vocab\n",
      "['▁행정', '안', '전', '부', '는', '▁오는', '▁20', '일부터', '▁', '버스', '·', '전', '철', '▁등', '▁대중', '교통', '과', '▁', '마트', '▁등', '▁대형', '시설', '▁안', '의', '▁개방', '형', '▁약', '국', '에서', '▁마', '스', '크', '▁', '착', '용', '▁', '의', '무', '를', '▁', '해', '제', '한다고', '▁', '밝', '혔', '습니다', '.']\n",
      "[2829, 208, 104, 110, 8, 410, 377, 948, 3, 4639, 24, 104, 748, 20, 4690, 3387, 21, 3, 4840, 20, 674, 2406, 340, 5, 2170, 197, 485, 285, 25, 508, 51, 315, 3, 1632, 94, 3, 5, 265, 10, 3, 38, 138, 1535, 3, 7980, 7971, 2750, 4]\n",
      "['▁전세계적으로', '▁‘', '챗', '봇', '▁돌풍', '’', '을', '▁', '일', '으', '킨', '▁생성', '형', '▁인공지능', '(', 'A', 'I', ')', '▁', '챗', 'G', 'P', 'T', '▁개발', '사', '인', '▁오픈', 'A', 'I', '가', '▁14', '일', '(', '현', '지', '시간', ')', '▁더욱', '▁강력', '해', '신', '▁새로운', '▁인공지능', '▁툴', '인', '▁G', 'P', 'T', '-', '4', '를', '▁공개', '했다', '.', '▁A', 'I', '▁기술을', '▁놓고', '▁구글과', '▁무', '한', '▁경쟁', '에', '▁돌입', '한', '▁마이크로소프트', '(', 'MS', ')', '는', '▁즉각', '▁자사', '▁검색엔진', '에', '▁G', 'P', 'T', '-', '4', '▁탑재', '를', '▁선언', ',', '▁구글과', '의', '▁격', '차', '▁벌', '리', '기', '에', '▁나섰다', '.']\n",
      "[4397, 30, 0, 2524, 5163, 28, 7, 3, 36, 7995, 7929, 3155, 197, 5119, 17, 277, 458, 16, 3, 0, 604, 408, 409, 71, 47, 26, 1550, 277, 458, 11, 785, 36, 17, 541, 22, 115, 16, 511, 4505, 38, 247, 253, 5119, 2827, 26, 2156, 408, 409, 194, 155, 10, 422, 42, 4, 1564, 458, 667, 1451, 1990, 374, 23, 494, 6, 1839, 23, 713, 17, 444, 16, 8, 3219, 949, 428, 6, 2156, 408, 409, 194, 155, 1653, 10, 2567, 12, 1990, 5, 2720, 273, 2371, 50, 32, 6, 995, 4]\n"
     ]
    }
   ],
   "source": [
    "!python testSPM-ko.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f46043f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'did', \"n't\", 'like', 'the', 'food', '.', 'She', 'never', 'did', '.', 'She', 'made', 'the', 'usual', 'complaints', 'and', 'started', 'the', 'tantrum', 'he', 'knew', 'was', 'coming', '.', 'But', 'this', 'time', 'was', 'different', '.', 'Instead', 'of', 'trying', 'to', 'placate', 'her', 'and', 'her', 'unreasonable', 'demands', ',', 'he', 'just', 'stared', 'at', 'her', 'and', 'watched', 'her', 'meltdown', 'without', 'saying', 'a', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Tokenizing\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# https://randomwordgenerator.com/paragraph.php\n",
    "s1 = \"She didn't like the food. She never did. She made the usual complaints and started the tantrum he knew was coming. But this time was different. Instead of trying to placate her and her unreasonable demands, he just stared at her and watched her meltdown without saying a word.\"\n",
    "w1 = word_tokenize(s1)\n",
    "\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9cc554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['may', 'take', 'lives', ',', \"'ll\", 'never', 'take', 'freedom', '!']\n"
     ]
    }
   ],
   "source": [
    "# Filtering Stop Words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "s2 = \"They may take our lives, but they'll never take our freedom!\"\n",
    "w2 = word_tokenize(s2)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "        \n",
    "filtered_list = [word for word in w2 if word.casefold() not in stop_words]\n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "638c2ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they', 'may', 'take', 'our', 'live', ',', 'but', 'they', \"'ll\", 'never', 'take', 'our', 'freedom', '!']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "s3 = \"As the years pass by, we all know owners look more and more like their dogs.\"\n",
    "w3 = word_tokenize(s2)\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in w3]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90cf442c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Peanut', 'NNP'),\n",
       " ('butter', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('jelly', 'RB'),\n",
       " ('caused', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('elderly', 'JJ'),\n",
       " ('lady', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('think', 'VB'),\n",
       " ('about', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('past', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tagging Parts of Speech\n",
    "\n",
    "s4 = \"Peanut butter and jelly caused the elderly lady to think about her past.\"\n",
    "w4 = word_tokenize(s4)\n",
    "\n",
    "nltk.pos_tag(w4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4365be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trash', 'covered', 'the', 'landscape', 'like', 'sprinkle', 'do', 'a', 'birthday', 'cake', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "s5 = \"Trash covered the landscape like sprinkles do a birthday cake.\"\n",
    "w5 = word_tokenize(s5)\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in w5]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08e0d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "s6 = \"I used to live in my neighbor's fishpond, but the aesthetic wasn't to my taste.\"\n",
    "w6 = word_tokenize(s6)\n",
    "pos_tags = nltk.pos_tag(w6)\n",
    "\n",
    "chunk_parser = nltk.RegexpParser( \"NP: {<DT>?<JJ>*<NN>}\")\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e81ec09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinking\n",
    "\n",
    "grammar = \"\"\"\n",
    "Chunk: {<.*>+}\n",
    "        }<JJ>{\"\"\"\n",
    "\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "tree2 = chunk_parser.parse(pos_tags)\n",
    "tree2.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5675aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Debbie', 'George'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Named Entity Recognition (NER)\n",
    "\n",
    "s7 = \"Debbie had taken George for granted for more than fifteen years now. He wasn't sure what exactly had made him choose this time and place to address the issue, but he decided that now was the time. He looked straight into her eyes and just as she was about to speak, turned away and walked out the door.\"\n",
    "\n",
    "def extract_ne(s):\n",
    "    words = word_tokenize(s)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    tree = nltk.ne_chunk(tags, binary=True)\n",
    "    return set(\n",
    "        \" \".join(i[0] for i in t)\n",
    "        for t in tree\n",
    "        if hasattr(t, \"label\") and t.label() == \"NE\"\n",
    "    )\n",
    "\n",
    "extract_ne(s7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
